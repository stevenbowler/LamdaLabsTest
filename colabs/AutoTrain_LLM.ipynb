{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenbowler/LamdaLabsTest/blob/master/colabs/AutoTrain_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "JvMRbVLEJlZT"
      },
      "outputs": [],
      "source": [
        "#@title 🤗 AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt1FBYAJSsVR",
        "outputId": "d8768df7-76a7-491b-af11-b32446331b7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/ColabStuff/train.csv\" \"/content/data/train.csv\"\n"
      ],
      "metadata": {
        "id": "B_fMIcEeTI_b"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "aM0GB6XAVWWR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'FineTuneLLaMa2' # @param {type:\"string\"}\n",
        "model_name = 'TinyPixel/Llama-2-7B-bf16-sharded' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"hf_Xhf_NtfjRYETXcSEVzgMabpSwayoosyraIKUsiXX\" #@param {type:\"string\"}\n",
        "repo_id = \"stevenbowler/FineTuneLLaMa2\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 32 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 1024 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "use_fp16 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "use_peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "use_int4 = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "lora_r = 16 #@param {type:\"number\"}\n",
        "lora_alpha = 32 #@param {type:\"number\"}\n",
        "lora_dropout = 0.05 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"PROJECT_NAME\"] = project_name\n",
        "os.environ[\"MODEL_NAME\"] = model_name\n",
        "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"REPO_ID\"] = repo_id\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
        "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
        "os.environ[\"USE_INT4\"] = str(use_int4)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install fine tuner\n",
        "!pip install autotrain-advanced\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "8VErCv8UWBbC",
        "outputId": "a95569f2-a0b6-4cd3-ee78-0ab1d0e8aa63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autotrain-advanced\n",
            "  Downloading autotrain_advanced-0.6.31-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations==1.3.1 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced) (1.3.1)\n",
            "Collecting codecarbon==2.2.3 (from autotrain-advanced)\n",
            "  Downloading codecarbon-2.2.3-py3-none-any.whl (174 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets[vision]~=2.14.0 (from autotrain-advanced)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.3.0 (from autotrain-advanced)\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic==1.0.0 (from autotrain-advanced)\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer==3.0.2 (from autotrain-advanced)\n",
            "  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Collecting joblib==1.3.1 (from autotrain-advanced)\n",
            "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru==0.7.0 (from autotrain-advanced)\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna==3.3.0 (from autotrain-advanced)\n",
            "  Downloading optuna-3.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.0.3 (from autotrain-advanced)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==10.0.0 (from autotrain-advanced)\n",
            "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==4.23.4 (from autotrain-advanced)\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.11 (from autotrain-advanced)\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.0.53 (from autotrain-advanced)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.3.0 (from autotrain-advanced)\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.99 (from autotrain-advanced)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.65.0 (from autotrain-advanced)\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug==2.3.6 (from autotrain-advanced)\n",
            "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xgboost==1.7.6 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced) (1.7.6)\n",
            "Collecting huggingface-hub>=0.16.4 (from autotrain-advanced)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced) (2.31.0)\n",
            "Collecting gradio==3.39.0 (from autotrain-advanced)\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.6.1 (from autotrain-advanced)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting invisible-watermark==0.2.0 (from autotrain-advanced)\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from autotrain-advanced) (2.12.3)\n",
            "Collecting peft (from autotrain-advanced)\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl (from autotrain-advanced)\n",
            "  Downloading trl-0.7.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from autotrain-advanced)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from autotrain-advanced)\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from autotrain-advanced)\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers (from autotrain-advanced)\n",
            "  Downloading diffusers-0.20.2.tar.gz (989 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.1/989.1 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes (from autotrain-advanced)\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.3.1->autotrain-advanced) (4.8.0.76)\n",
            "Collecting arrow (from codecarbon==2.2.3->autotrain-advanced)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynvml (from codecarbon==2.2.3->autotrain-advanced)\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced) (9.0.0)\n",
            "Collecting fuzzywuzzy (from codecarbon==2.2.3->autotrain-advanced)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon==2.2.3->autotrain-advanced) (8.1.7)\n",
            "Collecting dill (from evaluate==0.3.0->autotrain-advanced)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate==0.3.0->autotrain-advanced)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate==0.3.0->autotrain-advanced)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0->autotrain-advanced) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate==0.3.0->autotrain-advanced) (23.1)\n",
            "Collecting responses<0.19 (from evaluate==0.3.0->autotrain-advanced)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.3.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.39.0->autotrain-advanced) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced) (1.4.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced) (4.8.0.76)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible-watermark==0.2.0->autotrain-advanced) (2.0.1+cu118)\n",
            "Collecting rapidfuzz==2.13.7 (from jiwer==3.0.2->autotrain-advanced)\n",
            "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna==3.3.0->autotrain-advanced)\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.10.0 (from optuna==3.3.0->autotrain-advanced)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna==3.3.0->autotrain-advanced)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna==3.3.0->autotrain-advanced) (2.0.20)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->autotrain-advanced) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->autotrain-advanced) (2023.3)\n",
            "Collecting tzdata>=2022.1 (from pandas==2.0.3->autotrain-advanced)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->autotrain-advanced) (2023.7.22)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->autotrain-advanced) (2023.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.0.53->autotrain-advanced) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0->autotrain-advanced) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]~=2.14.0->autotrain-advanced) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->autotrain-advanced) (3.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->autotrain-advanced) (6.8.0)\n",
            "Collecting safetensors>=0.3.1 (from diffusers->autotrain-advanced)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (3.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->autotrain-advanced) (0.41.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->autotrain-advanced)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.39.0->autotrain-advanced) (1.3.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna==3.3.0->autotrain-advanced)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->autotrain-advanced) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->autotrain-advanced) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0->autotrain-advanced) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0->autotrain-advanced) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0->autotrain-advanced) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0->autotrain-advanced) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0->autotrain-advanced) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0->autotrain-advanced) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.39.0->autotrain-advanced) (3.1.1)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1->autotrain-advanced) (2023.8.12)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna==3.3.0->autotrain-advanced) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0->autotrain-advanced) (1.12)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->invisible-watermark==0.2.0->autotrain-advanced) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->invisible-watermark==0.2.0->autotrain-advanced) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->invisible-watermark==0.2.0->autotrain-advanced) (16.0.6)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.39.0->autotrain-advanced) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio==3.39.0->autotrain-advanced)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.39.0->autotrain-advanced) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->autotrain-advanced) (3.16.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.39.0->autotrain-advanced) (1.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0->autotrain-advanced) (0.9.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0->autotrain-advanced) (1.0.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->autotrain-advanced) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->autotrain-advanced) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible-watermark==0.2.0->autotrain-advanced) (1.3.0)\n",
            "Building wheels for collected packages: ipadic, sacremoses, diffusers, ffmpy\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=51f252ed7af7a77e99fff160708841b88c23f417bafb21ff712fbbfecb3c35db\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=ca629e0154d4dd70b4bcdca3bd0855c74d0635aec31e0a9521e7e63c57331da1\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.20.2-py3-none-any.whl size=1342633 sha256=11d1740c10f025f5cb41cbd602644c14d251fc6ce642eed1bb81b23ef037d547\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/8b/d9/34f7a1936109e05e9bba0cc2241a6f8cd89e25959dc7aae942\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=fa5e29388ec7f9e31a5996abd21403702e3baed6b79eb33666961859fcc32f6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ipadic sacremoses diffusers ffmpy\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pydub, ipadic, fuzzywuzzy, ffmpy, bitsandbytes, xxhash, werkzeug, websockets, tzdata, tqdm, semantic-version, rapidfuzz, python-multipart, pynvml, pydantic, protobuf, Pillow, orjson, markdown-it-py, Mako, loguru, joblib, h11, einops, dill, colorlog, cmaes, aiofiles, uvicorn, tiktoken, starlette, scikit-learn, sacremoses, responses, pandas, multiprocess, mdit-py-plugins, jiwer, huggingface-hub, httpcore, arrow, alembic, transformers, optuna, httpx, fastapi, diffusers, codecarbon, gradio-client, datasets, gradio, evaluate, accelerate, trl, peft, invisible-watermark, autotrain-advanced\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.3.7\n",
            "    Uninstalling Werkzeug-2.3.7:\n",
            "      Successfully uninstalled Werkzeug-2.3.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.2.1\n",
            "    Uninstalling pydantic-2.2.1:\n",
            "      Successfully uninstalled pydantic-2.2.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.2\n",
            "    Uninstalling joblib-1.3.2:\n",
            "      Successfully uninstalled joblib-1.3.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.2.4 Pillow-10.0.0 accelerate-0.22.0 aiofiles-23.2.1 alembic-1.12.0 arrow-1.2.3 autotrain-advanced-0.6.31 bitsandbytes-0.41.1 cmaes-0.10.0 codecarbon-2.2.3 colorlog-6.7.0 datasets-2.14.4 diffusers-0.20.2 dill-0.3.7 einops-0.6.1 evaluate-0.3.0 fastapi-0.103.1 ffmpy-0.3.1 fuzzywuzzy-0.18.0 gradio-3.39.0 gradio-client-0.5.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 invisible-watermark-0.2.0 ipadic-1.0.0 jiwer-3.0.2 joblib-1.3.1 loguru-0.7.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 multiprocess-0.70.15 optuna-3.3.0 orjson-3.9.5 pandas-2.0.3 peft-0.5.0 protobuf-4.23.4 pydantic-1.10.11 pydub-0.25.1 pynvml-11.5.0 python-multipart-0.0.6 rapidfuzz-2.13.7 responses-0.18.0 sacremoses-0.0.53 safetensors-0.3.3 scikit-learn-1.3.0 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.32.1 trl-0.7.1 tzdata-2023.3 uvicorn-0.23.2 websockets-11.0.3 werkzeug-2.3.6 xxhash-3.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!autotrain setup --update-torch"
      ],
      "metadata": {
        "id": "5arVTjkhV_og",
        "outputId": "1630566a-9565-47e5-bb4f-2d42435e8f87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[1mINFO    Installing latest transformers@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest transformers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest peft@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest peft\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest diffusers@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest diffusers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest trl@main\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest trl\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Installing latest PyTorch\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "g3cd_ED_yXXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a00c821-2e92-41c7-9a1e-c5b78aac84c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, train=True, deploy=False, inference=False, data_path='data/', train_split='train', valid_split=None, text_column='text', model='TinyPixel/Llama-2-7B-bf16-sharded', learning_rate=0.0002, num_train_epochs=1, train_batch_size=32, warmup_ratio=0.1, gradient_accumulation_steps=4, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, seed=42, add_eos_token=False, block_size=1024, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, project_name='FineTuneLLaMa2', evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, fp16=True, push_to_hub=True, use_int8=False, model_max_length=1024, repo_id='stevenbowler/FineTuneLLaMa2', use_int4=True, trainer='default', target_modules=None, merge_adapter=False, token='hf_Xhf_NtfjRYETXcSEVzgMabpSwayoosyraIKUsiXX', backend='default', username=None, func=<function run_llm_command_factory at 0x7b6174412200>)\u001b[0m\n",
            "> \u001b[1mINFO    loading dataset from csv\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)okenizer_config.json: 100% 676/676 [00:00<00:00, 3.10MB/s]\n",
            "Downloading tokenizer.model: 100% 500k/500k [00:00<00:00, 58.8MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 1.88MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 411/411 [00:00<00:00, 2.22MB/s]\n",
            "Using pad_token, but it is not set yet.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1006: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)lve/main/config.json: 100% 626/626 [00:00<00:00, 3.33MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 75.3MB/s]\n",
            "Downloading shards:   0% 0/14 [00:00<?, ?it/s]\n",
            "Downloading (…)l-00001-of-00014.bin:   0% 0.00/981M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:   5% 52.4M/981M [00:00<00:02, 449MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  12% 115M/981M [00:00<00:01, 498MB/s] \u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  18% 178M/981M [00:00<00:01, 511MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  25% 241M/981M [00:00<00:01, 524MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  31% 304M/981M [00:00<00:01, 533MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  37% 367M/981M [00:00<00:01, 536MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  44% 430M/981M [00:00<00:01, 536MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  50% 493M/981M [00:00<00:00, 533MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  57% 556M/981M [00:01<00:00, 528MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  63% 619M/981M [00:01<00:00, 523MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  68% 671M/981M [00:01<00:00, 517MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  74% 724M/981M [00:01<00:00, 508MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  79% 776M/981M [00:01<00:00, 484MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  84% 828M/981M [00:01<00:00, 454MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  90% 881M/981M [00:01<00:00, 436MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin:  95% 933M/981M [00:01<00:00, 443MB/s]\u001b[A\n",
            "Downloading (…)l-00001-of-00014.bin: 100% 981M/981M [00:02<00:00, 488MB/s]\n",
            "Downloading shards:   7% 1/14 [00:03<00:39,  3.01s/it]\n",
            "Downloading (…)l-00002-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:   5% 52.4M/967M [00:00<00:01, 461MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  11% 105M/967M [00:00<00:01, 493MB/s] \u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  16% 157M/967M [00:00<00:01, 505MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  22% 210M/967M [00:00<00:01, 512MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  27% 262M/967M [00:00<00:01, 499MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  33% 315M/967M [00:00<00:01, 482MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  38% 367M/967M [00:00<00:01, 482MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  43% 419M/967M [00:00<00:01, 473MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  49% 472M/967M [00:01<00:01, 436MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  54% 524M/967M [00:01<00:01, 422MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  60% 577M/967M [00:01<00:00, 423MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  65% 629M/967M [00:01<00:00, 427MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  70% 682M/967M [00:01<00:00, 439MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  76% 734M/967M [00:01<00:00, 445MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  81% 786M/967M [00:01<00:00, 448MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  87% 839M/967M [00:01<00:00, 452MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin:  92% 891M/967M [00:01<00:00, 436MB/s]\u001b[A\n",
            "Downloading (…)l-00002-of-00014.bin: 100% 967M/967M [00:02<00:00, 448MB/s]\n",
            "Downloading shards:  14% 2/14 [00:05<00:33,  2.77s/it]\n",
            "Downloading (…)l-00003-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:   5% 52.4M/967M [00:00<00:01, 482MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  12% 115M/967M [00:00<00:01, 524MB/s] \u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  18% 178M/967M [00:00<00:01, 543MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  25% 241M/967M [00:00<00:01, 555MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  31% 304M/967M [00:00<00:01, 541MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  38% 367M/967M [00:00<00:01, 545MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  44% 430M/967M [00:00<00:00, 558MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  51% 493M/967M [00:00<00:00, 569MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  57% 556M/967M [00:00<00:00, 574MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  64% 619M/967M [00:01<00:00, 506MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  69% 671M/967M [00:01<00:00, 425MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  75% 724M/967M [00:01<00:00, 397MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  79% 765M/967M [00:01<00:00, 395MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  84% 807M/967M [00:01<00:00, 368MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  88% 849M/967M [00:01<00:00, 357MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin:  92% 891M/967M [00:01<00:00, 355MB/s]\u001b[A\n",
            "Downloading (…)l-00003-of-00014.bin: 100% 967M/967M [00:02<00:00, 449MB/s]\n",
            "Downloading shards:  21% 3/14 [00:08<00:29,  2.69s/it]\n",
            "Downloading (…)l-00004-of-00014.bin:   0% 0.00/990M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:   5% 52.4M/990M [00:00<00:01, 486MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  11% 105M/990M [00:00<00:01, 507MB/s] \u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  17% 168M/990M [00:00<00:01, 516MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  23% 231M/990M [00:00<00:01, 523MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  30% 294M/990M [00:00<00:01, 532MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  36% 357M/990M [00:00<00:01, 520MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  41% 409M/990M [00:00<00:01, 496MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  47% 461M/990M [00:00<00:01, 431MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  52% 514M/990M [00:01<00:01, 394MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  57% 566M/990M [00:01<00:01, 396MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  61% 608M/990M [00:01<00:00, 390MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  66% 650M/990M [00:01<00:00, 370MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  70% 692M/990M [00:01<00:00, 367MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  74% 734M/990M [00:01<00:00, 351MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  78% 776M/990M [00:01<00:00, 333MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  83% 818M/990M [00:02<00:00, 268MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  86% 849M/990M [00:02<00:00, 246MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin:  90% 891M/990M [00:02<00:00, 280MB/s]\u001b[A\n",
            "Downloading (…)l-00004-of-00014.bin: 100% 990M/990M [00:02<00:00, 385MB/s]\n",
            "Downloading shards:  29% 4/14 [00:11<00:28,  2.81s/it]\n",
            "Downloading (…)l-00005-of-00014.bin:   0% 0.00/944M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:   6% 52.4M/944M [00:00<00:01, 461MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  12% 115M/944M [00:00<00:01, 506MB/s] \u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  19% 178M/944M [00:00<00:01, 491MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  24% 231M/944M [00:00<00:01, 389MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  29% 273M/944M [00:00<00:01, 372MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  33% 315M/944M [00:00<00:01, 344MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  38% 357M/944M [00:00<00:01, 334MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  42% 398M/944M [00:01<00:01, 334MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  47% 440M/944M [00:01<00:01, 334MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  51% 482M/944M [00:01<00:01, 334MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  56% 524M/944M [00:01<00:01, 319MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  60% 566M/944M [00:01<00:01, 317MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  64% 608M/944M [00:01<00:01, 322MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  69% 650M/944M [00:01<00:00, 321MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  73% 692M/944M [00:01<00:00, 318MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  78% 734M/944M [00:02<00:00, 314MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  82% 776M/944M [00:02<00:00, 315MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  87% 818M/944M [00:02<00:00, 335MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin:  92% 870M/944M [00:02<00:00, 365MB/s]\u001b[A\n",
            "Downloading (…)l-00005-of-00014.bin: 100% 944M/944M [00:02<00:00, 355MB/s]\n",
            "Downloading shards:  36% 5/14 [00:14<00:26,  2.91s/it]\n",
            "Downloading (…)l-00006-of-00014.bin:   0% 0.00/990M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:   4% 41.9M/990M [00:00<00:02, 417MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  10% 94.4M/990M [00:00<00:01, 450MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  15% 147M/990M [00:00<00:01, 439MB/s] \u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  20% 199M/990M [00:00<00:01, 412MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  25% 252M/990M [00:00<00:01, 428MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  31% 304M/990M [00:00<00:01, 421MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  36% 357M/990M [00:00<00:01, 434MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  41% 409M/990M [00:00<00:01, 436MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  47% 461M/990M [00:01<00:01, 459MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  53% 524M/990M [00:01<00:00, 487MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  59% 587M/990M [00:01<00:00, 503MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  66% 650M/990M [00:01<00:00, 521MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  72% 713M/990M [00:01<00:00, 539MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  78% 776M/990M [00:01<00:00, 558MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  85% 839M/990M [00:01<00:00, 574MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin:  91% 902M/990M [00:01<00:00, 584MB/s]\u001b[A\n",
            "Downloading (…)l-00006-of-00014.bin: 100% 990M/990M [00:01<00:00, 503MB/s]\n",
            "Downloading shards:  43% 6/14 [00:16<00:21,  2.74s/it]\n",
            "Downloading (…)l-00007-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:   5% 52.4M/967M [00:00<00:01, 501MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  12% 115M/967M [00:00<00:01, 551MB/s] \u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  18% 178M/967M [00:00<00:01, 575MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  25% 241M/967M [00:00<00:01, 580MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  31% 304M/967M [00:00<00:01, 412MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  37% 357M/967M [00:00<00:01, 312MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  41% 398M/967M [00:01<00:02, 269MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  46% 440M/967M [00:01<00:02, 243MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  49% 472M/967M [00:01<00:02, 235MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  52% 503M/967M [00:01<00:02, 229MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  55% 535M/967M [00:01<00:01, 229MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  59% 566M/967M [00:01<00:01, 241MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  63% 608M/967M [00:01<00:01, 279MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  68% 661M/967M [00:02<00:00, 338MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  73% 703M/967M [00:02<00:01, 245MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  79% 765M/967M [00:02<00:00, 313MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  86% 828M/967M [00:02<00:00, 372MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin:  92% 891M/967M [00:02<00:00, 415MB/s]\u001b[A\n",
            "Downloading (…)l-00007-of-00014.bin: 100% 967M/967M [00:02<00:00, 337MB/s]\n",
            "Downloading shards:  50% 7/14 [00:20<00:20,  2.92s/it]\n",
            "Downloading (…)l-00008-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:   5% 52.4M/967M [00:00<00:01, 458MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  11% 105M/967M [00:00<00:01, 451MB/s] \u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  16% 157M/967M [00:00<00:01, 441MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  22% 210M/967M [00:00<00:01, 401MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  26% 252M/967M [00:00<00:01, 376MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  30% 294M/967M [00:00<00:01, 360MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  35% 336M/967M [00:00<00:01, 348MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  39% 377M/967M [00:01<00:01, 336MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  43% 419M/967M [00:01<00:01, 328MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  48% 461M/967M [00:01<00:01, 321MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  52% 503M/967M [00:01<00:01, 329MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  56% 545M/967M [00:01<00:01, 335MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  61% 587M/967M [00:01<00:01, 344MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  66% 640M/967M [00:01<00:00, 374MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  72% 692M/967M [00:01<00:00, 412MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  77% 744M/967M [00:01<00:00, 438MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  82% 797M/967M [00:02<00:00, 456MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  88% 849M/967M [00:02<00:00, 466MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin:  93% 902M/967M [00:02<00:00, 474MB/s]\u001b[A\n",
            "Downloading (…)l-00008-of-00014.bin: 100% 967M/967M [00:02<00:00, 398MB/s]\n",
            "Downloading shards:  57% 8/14 [00:22<00:17,  2.92s/it]\n",
            "Downloading (…)l-00009-of-00014.bin:   0% 0.00/990M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:   4% 41.9M/990M [00:00<00:02, 404MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  10% 94.4M/990M [00:00<00:02, 415MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  14% 136M/990M [00:00<00:02, 412MB/s] \u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  19% 189M/990M [00:00<00:01, 423MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  24% 241M/990M [00:00<00:01, 450MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  30% 294M/990M [00:00<00:01, 468MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  35% 346M/990M [00:00<00:01, 477MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  41% 409M/990M [00:00<00:01, 501MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  48% 472M/990M [00:00<00:01, 508MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  53% 524M/990M [00:01<00:00, 507MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  58% 577M/990M [00:01<00:00, 498MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  64% 629M/990M [00:01<00:00, 490MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  69% 682M/990M [00:01<00:00, 482MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  74% 734M/990M [00:01<00:00, 486MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  79% 786M/990M [00:01<00:00, 496MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  86% 849M/990M [00:01<00:00, 508MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin:  92% 912M/990M [00:01<00:00, 513MB/s]\u001b[A\n",
            "Downloading (…)l-00009-of-00014.bin: 100% 990M/990M [00:02<00:00, 473MB/s]\n",
            "Downloading shards:  64% 9/14 [00:25<00:14,  2.80s/it]\n",
            "Downloading (…)l-00010-of-00014.bin:   0% 0.00/944M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:   6% 52.4M/944M [00:00<00:02, 436MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  11% 105M/944M [00:00<00:01, 450MB/s] \u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  17% 157M/944M [00:00<00:01, 453MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  22% 210M/944M [00:00<00:01, 475MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  28% 262M/944M [00:00<00:01, 472MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  33% 315M/944M [00:00<00:01, 463MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  39% 367M/944M [00:00<00:01, 462MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  44% 419M/944M [00:00<00:01, 473MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  50% 472M/944M [00:01<00:01, 468MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  56% 524M/944M [00:01<00:00, 471MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  61% 577M/944M [00:01<00:00, 473MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  67% 629M/944M [00:01<00:00, 450MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  72% 682M/944M [00:01<00:00, 400MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  77% 724M/944M [00:01<00:00, 385MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  81% 765M/944M [00:01<00:00, 379MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  86% 807M/944M [00:01<00:00, 370MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  90% 849M/944M [00:02<00:00, 353MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin:  94% 891M/944M [00:02<00:00, 332MB/s]\u001b[A\n",
            "Downloading (…)l-00010-of-00014.bin: 100% 944M/944M [00:02<00:00, 400MB/s]\n",
            "Downloading shards:  71% 10/14 [00:28<00:11,  2.81s/it]\n",
            "Downloading (…)l-00011-of-00014.bin:   0% 0.00/990M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:   5% 52.4M/990M [00:00<00:01, 487MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  12% 115M/990M [00:00<00:01, 518MB/s] \u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  18% 178M/990M [00:00<00:01, 527MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  24% 241M/990M [00:00<00:01, 529MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  31% 304M/990M [00:00<00:01, 530MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  37% 367M/990M [00:00<00:01, 539MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  43% 430M/990M [00:00<00:01, 416MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  49% 482M/990M [00:01<00:01, 344MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  53% 524M/990M [00:01<00:01, 309MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  57% 566M/990M [00:01<00:01, 289MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  60% 598M/990M [00:01<00:01, 268MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  64% 629M/990M [00:01<00:01, 250MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  67% 661M/990M [00:01<00:01, 232MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  70% 692M/990M [00:02<00:01, 221MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  73% 724M/990M [00:02<00:01, 233MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  77% 765M/990M [00:02<00:00, 268MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  82% 807M/990M [00:02<00:00, 297MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  87% 860M/990M [00:02<00:00, 342MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin:  92% 912M/990M [00:02<00:00, 382MB/s]\u001b[A\n",
            "Downloading (…)l-00011-of-00014.bin: 100% 990M/990M [00:02<00:00, 349MB/s]\n",
            "Downloading shards:  79% 11/14 [00:31<00:08,  2.95s/it]\n",
            "Downloading (…)l-00012-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:   4% 41.9M/967M [00:00<00:02, 409MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  10% 94.4M/967M [00:00<00:02, 414MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  15% 147M/967M [00:00<00:01, 432MB/s] \u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  21% 199M/967M [00:00<00:01, 429MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  26% 252M/967M [00:00<00:01, 400MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  31% 304M/967M [00:00<00:01, 411MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  36% 346M/967M [00:01<00:02, 268MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  41% 398M/967M [00:01<00:01, 311MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  47% 451M/967M [00:01<00:01, 341MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  51% 493M/967M [00:01<00:01, 347MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  55% 535M/967M [00:01<00:01, 363MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  61% 587M/967M [00:01<00:01, 379MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  65% 629M/967M [00:01<00:00, 376MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  69% 671M/967M [00:01<00:00, 365MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  74% 713M/967M [00:01<00:00, 373MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  79% 765M/967M [00:02<00:00, 400MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  85% 818M/967M [00:02<00:00, 424MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin:  90% 870M/967M [00:02<00:00, 441MB/s]\u001b[A\n",
            "Downloading (…)l-00012-of-00014.bin: 100% 967M/967M [00:02<00:00, 387MB/s]\n",
            "Downloading shards:  86% 12/14 [00:34<00:05,  2.95s/it]\n",
            "Downloading (…)l-00013-of-00014.bin:   0% 0.00/967M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:   4% 41.9M/967M [00:00<00:02, 375MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:   9% 83.9M/967M [00:00<00:02, 400MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  14% 136M/967M [00:00<00:01, 429MB/s] \u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  20% 189M/967M [00:00<00:01, 433MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  25% 241M/967M [00:00<00:01, 443MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  30% 294M/967M [00:00<00:01, 453MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  36% 346M/967M [00:00<00:01, 444MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  41% 398M/967M [00:00<00:01, 464MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  48% 461M/967M [00:01<00:01, 493MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  54% 524M/967M [00:01<00:00, 519MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  61% 587M/967M [00:01<00:00, 537MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  67% 650M/967M [00:01<00:00, 554MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  74% 713M/967M [00:01<00:00, 546MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  80% 776M/967M [00:01<00:00, 455MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  86% 828M/967M [00:01<00:00, 403MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  91% 881M/967M [00:01<00:00, 371MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin:  95% 923M/967M [00:02<00:00, 346MB/s]\u001b[A\n",
            "Downloading (…)l-00013-of-00014.bin: 100% 967M/967M [00:02<00:00, 418MB/s]\n",
            "Downloading shards:  93% 13/14 [00:37<00:02,  2.89s/it]\n",
            "Downloading (…)l-00014-of-00014.bin:   0% 0.00/847M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:   6% 52.4M/847M [00:00<00:01, 502MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  14% 115M/847M [00:00<00:01, 552MB/s] \u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  21% 178M/847M [00:00<00:01, 552MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  28% 241M/847M [00:00<00:01, 510MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  35% 294M/847M [00:00<00:01, 471MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  41% 346M/847M [00:00<00:01, 463MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  47% 398M/847M [00:00<00:01, 429MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  53% 451M/847M [00:00<00:00, 407MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  58% 493M/847M [00:01<00:00, 387MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  63% 535M/847M [00:01<00:00, 370MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  68% 577M/847M [00:01<00:00, 352MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  73% 619M/847M [00:01<00:00, 335MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  78% 661M/847M [00:01<00:00, 322MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  83% 703M/847M [00:01<00:00, 315MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  88% 744M/847M [00:01<00:00, 310MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin:  93% 786M/847M [00:02<00:00, 309MB/s]\u001b[A\n",
            "Downloading (…)l-00014-of-00014.bin: 100% 847M/847M [00:02<00:00, 373MB/s]\n",
            "Downloading shards: 100% 14/14 [00:39<00:00,  2.85s/it]\n",
            "Loading checkpoint shards: 100% 14/14 [00:25<00:00,  1.85s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)neration_config.json: 100% 132/132 [00:00<00:00, 595kB/s]\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32000. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Running tokenizer on train dataset:  88% 9000/10178 [00:01<00:00, 5422.76 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on train dataset: 100% 10178/10178 [00:01<00:00, 5130.30 examples/s]\n",
            "Grouping texts in chunks of 1024 (num_proc=4): 100% 10178/10178 [00:00<00:00, 10197.90 examples/s]\n",
            "> \u001b[1mINFO    creating trainer\u001b[0m\n",
            "  0% 0/16 [00:00<?, ?it/s]> \u001b[31m\u001b[1mERROR   train has failed due to an exception:\u001b[0m\n",
            "> \u001b[31m\u001b[1mERROR   Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/utils.py\", line 280, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autotrain/trainers/clm/__main__.py\", line 300, in train\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1553, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1835, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2679, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2704, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 632, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 620, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 931, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 94, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 820, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 701, in forward\n",
            "    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 697, in custom_forward\n",
            "    return module(*inputs, past_key_value, output_attentions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 424, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 362, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1845, in softmax\n",
            "    ret = input.softmax(dim, dtype=dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 14.75 GiB total capacity; 11.10 GiB already allocated; 2.38 GiB free; 11.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[0m\n",
            "  0% 0/16 [00:03<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path data/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
        "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
        "$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" ) \\\n",
        "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}